{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10604272,"sourceType":"datasetVersion","datasetId":6564226}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport xml.etree.ElementTree as ET\nimport random\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches\nfrom PIL import Image\nimport warnings\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as T\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler # For Mixed Precision\nimport timm # For ViT Enhancer\nfrom torch.optim.lr_scheduler import LambdaLR\nimport math\nfrom transformers import get_scheduler\n\n# Suppress warnings from torchvision NMS\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# ===== CONFIGURATION (Updated for YOLO Head) =====\nCONFIG = {\n    # Fusion & Backbone Architecture\n    'd_model': 512,\n    'nheads': 8,\n    'backbone_embed_dims': {\"layer4\": 512}, # \"layer2\": 128, \"layer3\": 256, \n    'backbone_feature_sizes': {\"layer4\": 16}, # \"layer2\": 64, \"layer3\": 32, \n    'img_size': 512,\n\n    # YOLO Architecture\n    'num_classes': 1,\n    'anchors': [\n        [[10, 13], [16, 30], [33, 23]],\n    ],\n    'yolo_strides': [32],\n\n    # ViT Enhancer\n    'vit_model_name': 'deit_small_patch16_224',\n\n    # Loss & Optimization\n    'lambda_deep_supervision': 0.1,\n    'optimizer': 'SGD',                   # NEW: Specify SGD optimizer\n    'lr': 0.001,                           # CHANGED: Adjusted for SGD\n    'lr_backbone': 1e-5,                 # CHANGED: Adjusted for SGD\n    'sgd_momentum': 0.937,                # NEW: Momentum for SGD\n    'dropout_rate': 0.2,                  # NEW: Dropout rate for fusion modules\n    'weight_decay': 5e-4,\n    'clip_norm_head': 10.0,\n    'clip_norm_backbone': 1.0,\n    'loss_box_weight': 0.05,               # changed from 0.05\n    'loss_obj_weight': 1.0,\n    'loss_cls_weight': 0.5,\n    'label_smoothing': 0.1,\n\n    # Training Settings\n    'batch_size': 8,\n    'num_epochs': 100,\n    'subset_size': 4000,\n    'ACCUMULATION_STEPS': 4,\n    'FREEZE_EPOCHS': 25,                  # CHANGED: Delayed unfreezing to epoch 50\n    'early_stop_patience': 20,            # 10 for now to get rid of the overfit first\n    'num_workers': 2,\n    'conf_threshold': 0.25,\n    'iou_threshold_nms': 0.45,\n}\n\n# ===== DATASET HANDLING =====\nclass LLVIPDataset(Dataset):\n    \"\"\"Loads LLVIP dataset pairs (RGB/Infrared) and their annotations.\"\"\"\n    def __init__(self, root_dir, split, subset_size=None, img_size=512):\n        self.root_dir = root_dir\n        self.img_size = img_size\n        self.is_train = split == 'train' # Set this attribute early\n        \n        self.rgb_dir = os.path.join(root_dir, 'LLVIP', 'visible', split)\n        self.ir_dir = os.path.join(root_dir, 'LLVIP', 'infrared', split)\n        self.ann_dir = os.path.join(root_dir, 'LLVIP', 'Annotations')\n\n        all_ids = [os.path.splitext(os.path.basename(p))[0] for p in glob.glob(os.path.join(self.rgb_dir, '*.jpg'))]\n        # Filter for IDs that are purely numeric to avoid any non-image files\n        self.image_ids = sorted([img_id for img_id in all_ids if img_id.isdigit()])\n\n        if subset_size and subset_size > 0:\n            self.image_ids = self.image_ids[:subset_size]\n            print(f\"Using subset of {len(self.image_ids)} images for split '{split}'.\")\n            \n        # Correctly define transforms based on the split\n        if self.is_train:\n            self.transform_rgb = T.Compose([\n                T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n                T.ToTensor(),\n                T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n            ])\n        else:\n            self.transform_rgb = T.Compose([\n                T.ToTensor(),\n                T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n            ])\n            \n        self.transform_ir = T.Compose([\n            T.ToTensor(),\n            T.Normalize([0.5], [0.5])\n        ])\n        self.resize = T.Resize((self.img_size, self.img_size))\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def _load_annotation(self, img_id, orig_w, orig_h):\n        ann_path = os.path.join(self.ann_dir, f\"{img_id}.xml\")\n        bboxes_norm = []\n        if not os.path.exists(ann_path):\n            return bboxes_norm\n        try:\n            tree = ET.parse(ann_path)\n            for obj in tree.getroot().findall('object'):\n                if obj.find('name').text.lower() == 'person':\n                    bbox = obj.find('bndbox')\n                    xmin = float(bbox.find('xmin').text) / orig_w\n                    ymin = float(bbox.find('ymin').text) / orig_h\n                    xmax = float(bbox.find('xmax').text) / orig_w\n                    ymax = float(bbox.find('ymax').text) / orig_h\n                    bboxes_norm.append([\n                        max(0.0, min(1.0, xmin)), max(0.0, min(1.0, ymin)),\n                        max(0.0, min(1.0, xmax)), max(0.0, min(1.0, ymax))\n                    ])\n        except ET.ParseError:\n            print(f\"Warning: Could not parse annotation {ann_path}\")\n        return bboxes_norm\n\n    def __getitem__(self, idx):\n        img_id = self.image_ids[idx]\n        rgb_path = os.path.join(self.rgb_dir, f\"{img_id}.jpg\")\n        ir_path = os.path.join(self.ir_dir, f\"{img_id}.jpg\")\n\n        try:\n            rgb_img = Image.open(rgb_path).convert('RGB')\n            ir_img = Image.open(ir_path).convert('L')\n            orig_w, orig_h = rgb_img.size\n\n            bboxes = self._load_annotation(img_id, orig_w, orig_h)\n            bboxes = torch.tensor(bboxes, dtype=torch.float32).view(-1, 4)\n\n            rgb_img = self.resize(rgb_img)\n            ir_img = self.resize(ir_img)\n\n            if self.is_train and random.random() < 0.5:\n                rgb_img = T.functional.hflip(rgb_img)\n                ir_img = T.functional.hflip(ir_img)\n                if bboxes.shape[0] > 0:\n                    bboxes[:, [0, 2]] = 1.0 - bboxes[:, [2, 0]]\n\n            return {\n                'rgb': self.transform_rgb(rgb_img),\n                'ir': self.transform_ir(ir_img),\n                'bboxes': bboxes, # Outputting xyxy boxes\n                'img_id': img_id\n            }\n        except Exception as e:\n            print(f\"Error loading image ID {img_id}: {e}\")\n            return None\n\n# ===== MODEL COMPONENTS (Backbone and Fusion - Unchanged) =====\n\nclass TimmResNetBackbone(nn.Module):\n    \"\"\" A wrapper for a Timm ResNet-18 model to extract multi-scale features. \"\"\"\n    def __init__(self, model_name='resnet18', pretrained=True):\n        super().__init__()\n        self.backbone = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            features_only=True,\n            out_indices=(2, 3, 4) # Corresponds to ResNet stages 2, 3, 4\n        )\n        self.feature_dims = self.backbone.feature_info.channels()\n        print(f\"Initialized {model_name} with feature dimensions: {self.feature_dims}\")\n\n    def forward(self, x):\n        features = self.backbone(x)\n        out = {\n            \"layer2\": features[0],\n            \"layer3\": features[1],\n            \"layer4\": features[2]\n        }\n        return out\n\nclass SimpleConvHead(nn.Module):\n    def __init__(self, in_channels, num_classes=2):\n        super().__init__()\n        # A simpler head for deep supervision, focusing only on classification\n        self.refine = nn.Sequential(\n            nn.Conv2d(in_channels, 256, 3, padding=1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True)\n        )\n        self.class_head = nn.Conv2d(256, num_classes, 1)\n\n    def forward(self, x):\n        # Only return logits, as this is just for providing a gradient signal\n        return {'pred_logits': self.class_head(self.refine(x))}\n\nclass TimmViTEnhancer(nn.Module):\n    \"\"\" Applies pretrained ViT blocks to input feature maps. \"\"\"\n    def __init__(self, embed_dim, img_size, model_name=CONFIG['vit_model_name'], pretrained=True):\n        super().__init__()\n        self.vit = timm.create_model(model_name, pretrained=pretrained)\n        self.img_size = img_size\n        self.vit.head = nn.Identity()\n        if hasattr(self.vit, 'norm'): self.vit.norm = nn.Identity()\n        self.input_proj = nn.Conv2d(embed_dim, self.vit.embed_dim, kernel_size=1) if embed_dim != self.vit.embed_dim else nn.Identity()\n        self.output_proj = nn.Conv2d(self.vit.embed_dim, embed_dim, kernel_size=1) if embed_dim != self.vit.embed_dim else nn.Identity()\n        self.norm = nn.LayerNorm(self.vit.embed_dim)\n        self.pos_embed_enhancer = nn.Parameter(torch.zeros(1, img_size*img_size, self.vit.embed_dim))\n        nn.init.trunc_normal_(self.pos_embed_enhancer, std=.02)\n\n    def forward(self, x):\n        B, C_in, H, W = x.shape\n        assert H == self.img_size and W == self.img_size, f\"Input size mismatch: {H}x{W} vs expected {self.img_size}x{self.img_size}\"\n        x_proj = self.input_proj(x)\n        C_vit = x_proj.shape[1]\n        tokens = x_proj.flatten(2).transpose(1, 2)\n        tokens = tokens + self.pos_embed_enhancer\n        tokens = self.vit.blocks(tokens)\n        tokens = self.norm(tokens)\n        tokens_spatial = tokens.transpose(1, 2).view(B, C_vit, H, W)\n        out = self.output_proj(tokens_spatial)\n        return out\n\nclass BiDirectionalCrossAttentionFusion(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout_rate=CONFIG['dropout_rate']):\n        super().__init__()\n        self.cross_attn_rgb = nn.MultiheadAttention(embed_dim, num_heads, batch_first=False)\n        self.cross_attn_ir = nn.MultiheadAttention(embed_dim, num_heads, batch_first=False)\n        self.query_proj_rgb = nn.Linear(embed_dim, embed_dim); self.key_proj_ir = nn.Linear(embed_dim, embed_dim); self.value_proj_ir = nn.Linear(embed_dim, embed_dim)\n        self.query_proj_ir = nn.Linear(embed_dim, embed_dim); self.key_proj_rgb = nn.Linear(embed_dim, embed_dim); self.value_proj_rgb = nn.Linear(embed_dim, embed_dim)\n        ffn_dim = embed_dim * 2\n        self.ffn_rgb = nn.Sequential(nn.Conv2d(embed_dim, ffn_dim, 1), nn.GELU(), nn.Dropout(dropout_rate), nn.Conv2d(ffn_dim, embed_dim, 1))\n        self.ffn_ir = nn.Sequential(nn.Conv2d(embed_dim, ffn_dim, 1), nn.GELU(), nn.Dropout(dropout_rate), nn.Conv2d(ffn_dim, embed_dim, 1))\n        self.norm_rgb_1 = nn.GroupNorm(1, embed_dim); self.norm_rgb_2 = nn.GroupNorm(1, embed_dim); self.norm_ir_1 = nn.GroupNorm(1, embed_dim); self.norm_ir_2 = nn.GroupNorm(1, embed_dim)\n        self.gate_conv = nn.Sequential(\n            nn.Conv2d(embed_dim * 2, embed_dim, 3, padding=1, bias=False), \n            nn.BatchNorm2d(embed_dim), \n            nn.ReLU(inplace=True), \n            nn.Conv2d(embed_dim, 4, 1)\n        )\n        \"\"\"# Add a call to the new initialization method\n        self._initialize_gate_bias()\n\n    def _initialize_gate_bias(self):\n        \n        Initializes the bias of the final conv layer in the gate\n        to give a higher initial weight to the IR features.\n        \n        # The 4 output channels of gate_conv correspond to weights for:\n        # [fused_rgb, fused_ir, original_rgb, original_ir]\n        with torch.no_grad():\n            self.gate_conv[-1].bias.fill_(0.0)  # Start with a neutral bias for RGB\n            self.gate_conv[-1].bias[1] = 1.0  # Give a positive bias to the fused_ir channel\n            self.gate_conv[-1].bias[3] = 1.0  # Give a positive bias to the original_ir channel\n            # This will cause the softmax to initially assign more weight to IR features.\"\"\"\n            \n    def forward(self, feat_rgb, feat_ir):\n        B, C, H, W = feat_rgb.shape\n        def to_seq(feat): return feat.flatten(2).permute(2, 0, 1)\n        def from_seq(seq): return seq.permute(1, 2, 0).view(B, C, H, W)\n        rgb_seq, ir_seq = to_seq(feat_rgb), to_seq(feat_ir)\n        Q_rgb = self.query_proj_rgb(rgb_seq); K_ir, V_ir = self.key_proj_ir(ir_seq), self.value_proj_ir(ir_seq)\n        attn_out_rgb, _ = self.cross_attn_rgb(Q_rgb, K_ir, V_ir)\n        x_rgb_res1 = feat_rgb + from_seq(attn_out_rgb); x_rgb_norm1 = self.norm_rgb_1(x_rgb_res1)\n        ffn_out_rgb = self.ffn_rgb(x_rgb_norm1); fused_rgb_processed = self.norm_rgb_2(x_rgb_norm1 + ffn_out_rgb)\n        Q_ir = self.query_proj_ir(ir_seq); K_rgb, V_rgb = self.key_proj_rgb(rgb_seq), self.value_proj_rgb(rgb_seq)\n        attn_out_ir, _ = self.cross_attn_ir(Q_ir, K_rgb, V_rgb)\n        x_ir_res1 = feat_ir + from_seq(attn_out_ir); x_ir_norm1 = self.norm_ir_1(x_ir_res1)\n        ffn_out_ir = self.ffn_ir(x_ir_norm1); fused_ir_processed = self.norm_ir_2(x_ir_norm1 + ffn_out_ir)\n        gate_input = torch.cat([feat_rgb, feat_ir], dim=1); spatial_weights = F.softmax(self.gate_conv(gate_input), dim=1)\n        w_fused_rgb, w_fused_ir, w_res_rgb, w_res_ir = spatial_weights.split(1, dim=1)\n        fused_output = (w_fused_rgb * fused_rgb_processed +\n                w_fused_ir * fused_ir_processed +\n                w_res_rgb * feat_rgb +\n                w_res_ir * feat_ir)\n        return fused_output, spatial_weights\n\nclass AxialAttention(nn.Module):\n    \"\"\" Performs attention along a single axis (height or width). \"\"\"\n    def __init__(self, embed_dim, num_heads, axis): super().__init__(); self.axis = axis; self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=False)\n    def forward(self, x):\n        B, C, H, W = x.shape\n        if self.axis == 1: x_perm = x.permute(3, 0, 2, 1).reshape(W * B, H, C).permute(1, 0, 2); attn_out, _ = self.attn(x_perm, x_perm, x_perm); out = attn_out.permute(1, 0, 2).reshape(W, B, H, C).permute(1, 3, 2, 0)\n        else: x_perm = x.permute(2, 0, 3, 1).reshape(H * B, W, C).permute(1, 0, 2); attn_out, _ = self.attn(x_perm, x_perm, x_perm); out = attn_out.permute(1, 0, 2).reshape(H, B, W, C).permute(1, 3, 0, 2)\n        return out\n\nclass AxialAttentionBlock(nn.Module):\n    \"\"\" A block applying axial attention sequentially along height and width, with FFN and residuals. \"\"\"\n    def __init__(self, embed_dim, num_heads, dropout_rate=CONFIG['dropout_rate']):\n        super().__init__()\n        self.axial_attn_height = AxialAttention(embed_dim, num_heads, axis=1); self.axial_attn_width = AxialAttention(embed_dim, num_heads, axis=2)\n        self.norm1 = nn.GroupNorm(1, embed_dim); ffn_dim = embed_dim * 2\n        self.ffn = nn.Sequential(nn.Conv2d(embed_dim, ffn_dim, 1), nn.GELU(), nn.Conv2d(ffn_dim, embed_dim, 1)); self.norm2 = nn.GroupNorm(1, embed_dim)\n    def forward(self, x):\n        residual1 = x; out_h = self.axial_attn_height(x); out_w = self.axial_attn_width(out_h)\n        x = self.norm1(residual1 + out_w); residual2 = x; out_ffn = self.ffn(x); x = self.norm2(residual2 + out_ffn)\n        return x\n\nclass MultiLevelFusion(nn.Module):\n    \"\"\" Fuses two feature maps using Cross-Attention and optionally Axial Attention. \"\"\"\n    def __init__(self, embed_dim, num_heads, use_axial: bool = True, dropout_rate=CONFIG['dropout_rate']):\n        super().__init__()\n        self.cross_attn = BiDirectionalCrossAttentionFusion(embed_dim, num_heads); self.use_axial = use_axial\n        if self.use_axial: self.axial_block = AxialAttentionBlock(embed_dim, num_heads); print(f\"MultiLevelFusion: Initialized WITH AxialAttention (dim={embed_dim})\")\n        else: self.axial_block = nn.Identity()\n    def forward(self, feat1, feat2):\n        # 1. Get both the fused features and the attention weights from the cross-attention module\n        cross_attn_output, spatial_weights = self.cross_attn(feat1, feat2)\n        \n        # 2. Apply axial attention only to the feature map\n        final_output = self.axial_block(cross_attn_output)\n        \n        # 3. Return both the final output and the passthrough weights for visualization\n        return final_output, spatial_weights\n\nclass EnhancedHybridFeatureExtractor(nn.Module):\n    \"\"\"\n    Uses TimmResNetBackbone and progressive fusion, with a corrected\n    deep supervision path.\n    \"\"\"\n    def __init__(self,\n                 embed_dims=CONFIG['backbone_embed_dims'],\n                 feature_sizes=CONFIG['backbone_feature_sizes'],\n                 num_heads=CONFIG['nheads'],\n                 d_model_head=CONFIG['d_model'],\n                 use_vit_enhancer=False,\n                 use_axial_fusion=True,\n                 dropout_rate=CONFIG['dropout_rate']):\n        super().__init__()\n        print(f\"Initializing PROGRESSIVE FUSION Feature Extractor: ViT Enhancer={'ON' if use_vit_enhancer else 'OFF'}\")\n        self.use_vit_enhancer = use_vit_enhancer\n        self.scales = list(embed_dims.keys())\n        \n        # --- Backbone Modules ---\n        self.backbone_rgb = TimmResNetBackbone()\n        self.backbone_ir = TimmResNetBackbone()\n        \n        # Adapt IR backbone for 1-channel input\n        original_conv1_weights = self.backbone_ir.backbone.conv1.weight.clone()\n        new_conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        new_conv1.weight.data = original_conv1_weights.mean(dim=1, keepdim=True)\n        self.backbone_ir.backbone.conv1 = new_conv1\n        print(\"Adapted IR backbone's first convolutional layer for 1-channel input.\")\n        \n        # --- Optional Enhancement & Intra-Modal Fusion Modules ---\n        if self.use_vit_enhancer:\n            self.shared_vit_enhancers = nn.ModuleDict()\n            self.fusions_intra_rgb = nn.ModuleDict()\n            self.fusions_intra_ir = nn.ModuleDict()\n            for scale in self.scales:\n                dim, size = embed_dims[scale], feature_sizes[scale]\n                self.shared_vit_enhancers[scale] = TimmViTEnhancer(dim, img_size=size)\n                self.fusions_intra_rgb[scale] = MultiLevelFusion(dim, num_heads, use_axial=use_axial_fusion)\n                self.fusions_intra_ir[scale] = MultiLevelFusion(dim, num_heads, use_axial=use_axial_fusion)\n\n        # --- Progressive Cross-Modal Fusion Modules ---\n        self.progressive_fusions = nn.ModuleDict({\n            scale: MultiLevelFusion(\n                embed_dims[scale], \n                num_heads, \n                use_axial=use_axial_fusion, \n                dropout_rate=dropout_rate\n            )\n            for scale in self.scales\n        })\n        \n        # --- Deep Supervision Head ---\n        # The input channels should match the backbone's output channel depth\n        self.deep_supervision_head = SimpleConvHead(\n            in_channels=self.backbone_rgb.feature_dims[-1],\n            num_classes=2\n        )\n\n    def freeze_backbone(self, freeze=True):\n        \"\"\"Set requires_grad for all backbone parameters.\"\"\"\n        print(f\"Setting backbone requires_grad = {not freeze}\")\n        for param in self.backbone_rgb.parameters():\n            param.requires_grad = not freeze\n        for param in self.backbone_ir.parameters():\n            param.requires_grad = not freeze\n\n    def unfreeze_backbone_layer(self, layer_name: str):\n        \"\"\"Unfreezes a specific layer of the ResNet backbones.\"\"\"\n        if layer_name not in [\"layer2\", \"layer3\", \"layer4\"]:\n            print(f\"Warning: Invalid layer name '{layer_name}' for unfreezing.\")\n            return\n        print(f\"--- Unfreezing backbone layer: {layer_name} ---\")\n        for name, param in self.backbone_rgb.named_parameters():\n            if layer_name in name:\n                param.requires_grad = True\n        for name, param in self.backbone_ir.named_parameters():\n            if layer_name in name:\n                param.requires_grad = True\n\n    def forward(self, rgb, ir):\n        # 1. Get the features from both backbones\n        rgb_features_by_layer = self.backbone_rgb(rgb)\n        ir_features_by_layer = self.backbone_ir(ir)\n\n        # 2. Apply deep supervision\n        deep_supervision_preds = self.deep_supervision_head(rgb_features_by_layer[\"layer4\"])\n        \n        # 3. Perform the fusion steps\n        final_fused_outputs = {}\n        spatial_weights_outputs = {} # Dict to store weights\n\n        for scale in self.scales:\n            rgb_feat_raw = rgb_features_by_layer[scale]\n            ir_feat_raw = ir_features_by_layer[scale]\n            \n            processed_rgb = rgb_feat_raw\n            processed_ir = ir_feat_raw\n\n            if self.use_vit_enhancer:\n                # This part has a separate issue, see the note below\n                vit_enhanced_rgb = self.shared_vit_enhancers[scale](rgb_feat_raw) + rgb_feat_raw\n                vit_enhanced_ir = self.shared_vit_enhancers[scale](ir_feat_raw) + ir_feat_raw\n                processed_rgb = self.fusions_intra_rgb[scale](rgb_feat_raw, vit_enhanced_rgb)\n                processed_ir = self.fusions_intra_ir[scale](ir_feat_raw, vit_enhanced_ir)\n            \n            # This is the corrected logic for the main fusion path\n            fused_feature, weights = self.progressive_fusions[scale](processed_rgb, processed_ir)\n            final_fused_outputs[scale] = fused_feature\n            spatial_weights_outputs[scale] = weights # Make sure 'weights' here matches the line above\n\n        # 4. Prepare final outputs\n        main_head_input = final_fused_outputs[\"layer4\"]\n\n        return main_head_input, deep_supervision_preds, spatial_weights_outputs\n\nclass YOLOHead(nn.Module):\n    \"\"\"\n    A simple YOLOv3-style detection head with separate convolutional layers\n    for box, objectness, and class predictions to improve stability.\n    \"\"\"\n    def __init__(self, in_channels, num_classes, anchors, stride):\n        super().__init__()\n        self.num_classes = num_classes\n        self.num_anchors = len(anchors)\n        self.stride = stride\n\n        # --- Create separate prediction heads for each task ---\n        # Head for bounding box regression (x, y, w, h)\n        self.conv_box = nn.Conv2d(in_channels, self.num_anchors * 4, kernel_size=1)\n        # Head for objectness score (is an object present?)\n        self.conv_obj = nn.Conv2d(in_channels, self.num_anchors * 1, kernel_size=1)\n        # Head for class probabilities\n        self.conv_cls = nn.Conv2d(in_channels, self.num_anchors * self.num_classes, kernel_size=1)\n\n        # Initialize the biases for the new heads\n        self._initialize_biases()\n\n        # Register anchors as buffers\n        self.register_buffer('anchors', torch.tensor(anchors).float().view(self.num_anchors, 2) / self.stride)\n        self.register_buffer('anchor_grid', self.anchors.clone().view(1, -1, 1, 1, 2))\n\n    def _initialize_biases(self):\n        # Initialize the objectness head bias to encourage object prediction\n        # log(p / (1-p)) -> for p=0.01, bias is approx -4.59\n        self.conv_obj.bias.data.fill_(-4.59)\n        \n        # Initialize class head bias for better starting point\n        if self.num_classes > 0:\n            # log(1/num_classes) is a good starting point\n            initial_bias = -math.log((1 - 0.01) / 0.01 / (self.num_classes - 1)) if self.num_classes > 1 else 0\n            self.conv_cls.bias.data.fill_(initial_bias)\n\n    def forward(self, x):\n        # x is the feature map from the backbone, e.g., [B, 512, 16, 16]\n        B, _, H, W = x.shape\n\n        # Get predictions from each separate head\n        pred_box = self.conv_box(x) # [B, num_anchors * 4, H, W]\n        pred_obj = self.conv_obj(x) # [B, num_anchors * 1, H, W]\n        pred_cls = self.conv_cls(x) # [B, num_anchors * num_classes, H, W]\n\n        # Reshape and concatenate the predictions\n        pred_box = pred_box.view(B, self.num_anchors, 4, H, W).permute(0, 3, 4, 1, 2).contiguous()\n        pred_obj = pred_obj.view(B, self.num_anchors, 1, H, W).permute(0, 3, 4, 1, 2).contiguous()\n        pred_cls = pred_cls.view(B, self.num_anchors, self.num_classes, H, W).permute(0, 3, 4, 1, 2).contiguous()\n\n        # Final prediction tensor format: [B, H, W, num_anchors, 5 + num_classes]\n        pred = torch.cat([pred_box, pred_obj, pred_cls], dim=-1)\n\n        return pred\n\nclass YOLOLoss(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.num_classes = config['num_classes']\n        self.stride = config['yolo_strides'][0]\n        \n        # Scale anchors to the feature map grid size\n        scaled_anchors = torch.tensor(config['anchors'][0]).float() / self.stride\n        self.register_buffer('anchors', scaled_anchors)\n        \n        self.num_anchors = len(self.anchors)\n        self.balance = [4.0] \n        self.bce_cls = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.0]))\n        self.bce_obj = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([self.balance[0]]))\n        self.box_weight = config['loss_box_weight']\n        self.obj_weight = config['loss_obj_weight']\n        self.cls_weight = config['loss_cls_weight']\n        self.anchor_t = 4.0\n        # Get label smoothing value from config\n        self.label_smoothing = config.get('label_smoothing', 0.0)\n\n    def forward(self, preds, targets):\n        device = preds.device\n        lbox, lobj, lcls = torch.zeros(1, device=device), torch.zeros(1, device=device), torch.zeros(1, device=device)\n\n        p = preds\n        tcls, tbox, indices, anchors = self.build_targets(p, targets.to(device))\n\n        tcls = tcls.to(device)\n        tbox = tbox.to(device)\n        indices = tuple(i.to(device) for i in indices)\n        anchors = anchors.to(device)\n\n        B, H, W, A, _ = p.shape\n        tobj = torch.zeros_like(p[..., 4], device=device) # Target objectness\n\n        b, a, gj, gi = indices\n        if b.shape[0] > 0:\n            ps = p[b, gj, gi, a] \n\n            # Box Loss (CIoU)\n            pxy = ps[:, :2].sigmoid() * 2 - 0.5\n            pwh = (ps[:, 2:4].sigmoid() * 2) ** 2 * anchors\n            pbox = torch.cat((pxy, pwh), 1)\n            iou = bbox_iou(pbox, tbox, x1y1x2y2=False, CIoU=True)\n            lbox += (1.0 - iou).mean()\n\n            # Objectness target is a hard 1.0 for matched anchors\n            tobj[b, gj, gi, a] = 1.0\n\n            # Classification Loss with Label Smoothing\n            if self.num_classes > 0:\n                # Create target tensor with smoothed labels\n                t = torch.full_like(ps[:, 5:], self.label_smoothing / (self.num_classes - 1) if self.num_classes > 1 else 0, device=device)\n                t[range(b.shape[0]), tcls.long()] = 1.0 - self.label_smoothing\n                lcls += self.bce_cls(ps[:, 5:], t)\n\n        # Objectness loss for all anchors (positive and negative)\n        lobj += self.bce_obj(p[..., 4], tobj)\n\n        total_loss = self.box_weight * lbox + self.obj_weight * lobj + self.cls_weight * lcls\n        return {\n            'total_loss': total_loss * B,\n            'loss_box': lbox * B,\n            'loss_obj': lobj * B,\n            'loss_cls': lcls * B\n        }\n\n    def build_targets(self, p, targets):\n        na = self.num_anchors\n        nt = targets.shape[0]\n        tcls, tbox, indices, anch = [], [], [], []\n        gain = torch.ones(7, device=targets.device)\n        \n        ai = torch.arange(na, device=targets.device).float().view(na, 1)\n        if nt:\n            targets = torch.cat((targets.repeat(na, 1, 1), ai.repeat(1, nt)[:, :, None]), 2)\n        else:\n            device = p.device\n            return torch.tensor([], device=device, dtype=torch.long), torch.tensor([], device=device), \\\n                   (torch.tensor([], device=device, dtype=torch.long), torch.tensor([], device=device, dtype=torch.long), \\\n                    torch.tensor([], device=device, dtype=torch.long), torch.tensor([], device=device, dtype=torch.long)), \\\n                   torch.tensor([], device=device)\n\n        g = 0.5\n        off = torch.tensor([[0, 0], [1, 0], [0, 1], [-1, 0], [0, -1]], device=targets.device).float() * g\n\n        anchors = self.anchors.to(targets.device)\n        gain[2:6] = torch.tensor(p.shape)[[1, 0, 1, 0]]\n\n        t = targets * gain\n        if nt:\n            r = t[:, :, 4:6] / anchors[:, None]\n            j = torch.max(r, 1. / r).max(2)[0] < self.anchor_t\n            t = t[j]\n\n        if t.shape[0] == 0:\n            device = p.device\n            return torch.tensor([], device=device, dtype=torch.long), torch.tensor([], device=device), \\\n                   (torch.tensor([], device=device, dtype=torch.long), torch.tensor([], device=device, dtype=torch.long), \\\n                    torch.tensor([], device=device, dtype=torch.long), torch.tensor([], device=device, dtype=torch.long)), \\\n                   torch.tensor([], device=device)\n\n        gxy = t[:, 2:4]\n        gxi = gain[2:4] - gxy\n        j, k = ((gxy % 1. < g) & (gxy > 1.)).T\n        l, m = ((gxi % 1. < g) & (gxi > 1.)).T\n        j = torch.stack((torch.ones_like(j), j, k, l, m))\n        t = t.repeat((5, 1, 1))[j]\n        offsets = (torch.zeros_like(gxy)[None] + off[:, None])[j]\n\n        b, c = t[:, :2].long().T\n        gxy = t[:, 2:4]\n        gwh = t[:, 4:6]\n        gij = (gxy - offsets).long()\n        gi, gj = gij.T\n\n        a = t[:, 6].long()\n        indices.append((b, a, gj.clamp_(0, int(gain[3].item()) - 1), gi.clamp_(0, int(gain[2].item()) - 1)))\n        tbox.append(torch.cat((gxy - gij, gwh), 1))\n        anch.append(anchors[a])\n        tcls.append(c)\n\n        return torch.cat(tcls, 0), torch.cat(tbox, 0), indices[0], torch.cat(anch, 0)\n\n\n# ===== UTILITIES (Visualization, Gradients, Dataloader, Box ops) =====\n\ndef box_cxcywh_to_xyxy(x):\n    x_c, y_c, w, h = x.unbind(-1)\n    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n    return torch.stack(b, dim=-1).clamp(min=0.0, max=1.0)\n\ndef box_xyxy_to_cxcywh(x):\n    x0, y0, x1, y1 = x.unbind(-1)\n    b = [(x0 + x1) / 2, (y0 + y1) / 2, (x1 - x0), (y1 - y0)]\n    return torch.stack(b, dim=-1)\n\ndef bbox_iou(box1, box2, x1y1x2y2=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-7):\n    \"\"\"\n    Returns the IoU of box1 to box2.\n    box1 and box2 are expected to be of shape [N, 4] for element-wise calculation.\n    \"\"\"\n    # Get the coordinates of bounding boxes\n    if x1y1x2y2:\n        # Unbind the last dimension to get (x1, y1, x2, y2)\n        b1_x1, b1_y1, b1_x2, b1_y2 = box1.unbind(-1)\n        b2_x1, b2_y1, b2_x2, b2_y2 = box2.unbind(-1)\n    else:  # transform from cxcywh to xyxy\n        # Use slicing for batch operations\n        b1_x1, b1_y1 = box1[:, 0] - box1[:, 2] / 2, box1[:, 1] - box1[:, 3] / 2\n        b1_x2, b1_y2 = box1[:, 0] + box1[:, 2] / 2, box1[:, 1] + box1[:, 3] / 2\n        b2_x1, b2_y1 = box2[:, 0] - box2[:, 2] / 2, box2[:, 1] - box2[:, 3] / 2\n        b2_x2, b2_y2 = box2[:, 0] + box2[:, 2] / 2, box2[:, 1] + box2[:, 3] / 2\n\n    # Intersection area\n    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n\n    # Union Area\n    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n    union = w1 * h1 + w2 * h2 - inter + eps\n\n    iou = inter / union\n    if CIoU or DIoU or GIoU:\n        cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)  # convex width\n        ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)  # convex height\n        if CIoU or DIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1\n            c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n            rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 +\n                    (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4  # center distance squared\n            if DIoU:\n                return iou - rho2 / c2  # DIoU\n            elif CIoU:  # Complete IoU\n                v = (4 / math.pi ** 2) * torch.pow(torch.atan(w2 / h2) - torch.atan(w1 / h1), 2)\n                with torch.no_grad():\n                    alpha = v / (v - iou + (1 + eps))\n                return iou - (rho2 / c2 + v * alpha)  # CIoU\n        else:  # GIoU https://arxiv.org/pdf/1902.09630.pdf\n            c_area = cw * ch + eps  # convex area\n            return iou - (c_area - union) / c_area  # GIoU\n    return iou\n\ndef non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False, max_det=300):\n    \"\"\"Performs Non-Maximum Suppression (NMS) on inference results.\"\"\"\n    # prediction: [B, H*W*num_anchors, 5+num_classes]\n    nc = prediction.shape[2] - 5  # number of classes\n    xc = prediction[..., 4] > conf_thres  # candidates\n\n    output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n    for xi, x in enumerate(prediction):  # image index, image inference\n        x = x[xc[xi]]  # confidence\n\n        if not x.shape[0]:\n            continue\n\n        box = box_cxcywh_to_xyxy(x[:, :4])\n        if multi_label:\n            i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n            x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n        else:  # best class only\n            conf, j = x[:, 5:].max(1, keepdim=True)\n            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n\n        if not x.shape[0]:\n            continue\n\n        c = x[:, 5:6] * (0 if agnostic else 4096)  # classes\n        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n        if i.shape[0] > max_det:  # limit detections\n            i = i[:max_det]\n\n        output[xi] = x[i]\n\n    return output\n\ndef prepare_deep_supervision_targets(targets, feature_map_size, device):\n    \"\"\" Creates target tensors for the dense deep supervision head. \"\"\"\n    B = len(targets)\n    H = W = feature_map_size\n    \n    # Create tensors directly on the target device\n    target_labels = torch.full((B, H, W), 0, dtype=torch.long, device=device)\n    target_boxes = torch.zeros((B, 4, H, W), dtype=torch.float32, device=device)\n\n    for i in range(B):\n        # It's safer to work with CPU tensors for the looping logic\n        gt_boxes_xyxy = targets[i]['gt_boxes_xyxy'].cpu()\n        if gt_boxes_xyxy.shape[0] == 0:\n            continue\n            \n        gt_boxes_grid = gt_boxes_xyxy * H\n        gt_boxes_cxcywh = box_xyxy_to_cxcywh(gt_boxes_xyxy)\n        for j, box_grid in enumerate(gt_boxes_grid):\n            xmin, ymin, xmax, ymax = box_grid.long()\n            xmin, ymin = max(0, xmin), max(0, ymin)\n            xmax, ymax = min(W - 1, xmax), min(H - 1, ymax)\n            if xmin > xmax or ymin > ymax: continue\n            \n            # Assign positive class and box targets to the tensors already on the correct device\n            target_labels[i, ymin:ymax+1, xmin:xmax+1] = 1 # Use label 1 for 'person'\n            target_boxes[i, :, ymin:ymax+1, xmin:xmax+1] = gt_boxes_cxcywh[j].unsqueeze(-1).unsqueeze(-1)\n\n    return target_labels, target_boxes\n\ndef visualize_boxes(rgb_tensor, pred_boxes_xyxy, gt_boxes_xyxy, score_thresh=0.5, denormalize=True, title_suffix=\"\"):\n    \"\"\" Displays an image with predicted and ground truth boxes. \"\"\"\n    if denormalize:\n        mean=np.array([0.485,0.456,0.406]); std=np.array([0.229,0.224,0.225])\n        img = np.clip(((rgb_tensor.cpu().numpy().transpose(1,2,0)*std)+mean),0,1)\n    else: img = rgb_tensor.cpu().numpy().transpose(1,2,0)\n    H,W,_ = img.shape; fig,ax = plt.subplots(figsize=(8,8)); ax.imshow(img); ax.axis('off')\n    # Draw predicted boxes (red)\n    for box in pred_boxes_xyxy:\n        score = box[4]\n        if score > score_thresh:\n            xmin,ymin,xmax,ymax = box[:4]*torch.tensor([W,H,W,H],device=box.device)\n            rect=patches.Rectangle((xmin.item(),ymin.item()),(xmax-xmin).item(),(ymax-ymin).item(),lw=2,edgecolor='r',facecolor='none')\n            ax.add_patch(rect); ax.text(xmin.item(),ymin.item()-5,f'{score:.2f}',color='r',fontsize=8,bbox=dict(facecolor='white',alpha=0.7,pad=0.1))\n    # Draw ground truth boxes (green)\n    for box in gt_boxes_xyxy.cpu():\n        xmin,ymin,xmax,ymax = box*torch.tensor([W,H,W,H],device=box.device)\n        rect=patches.Rectangle((xmin.item(),ymin.item()),(xmax-xmin).item(),(ymax-ymin).item(),lw=2,edgecolor='g',facecolor='none')\n        ax.add_patch(rect)\n    ax.set_title(f\"Pred(r,>{score_thresh}) vs GT(g) {title_suffix}\"); plt.show()\n\ndef print_grad_norms(model, name):\n    \"\"\" Prints total L2 norm of gradients for named parameters. \"\"\"\n    norm_sq=0; count=0; model_ = model.module if isinstance(model,nn.DataParallel) else model\n    for _,p in model_.named_parameters():\n        if p.grad is not None and p.requires_grad: norm_sq+=p.grad.norm(2).item()**2; count+=1\n    if count>0: print(f\"--> {name} grad norm: {norm_sq**0.5:.4f} ({count} params with grad)\")\n    else: print(f\"--> {name}: No parameters with gradients found.\")\n\ndef custom_collate_fn(batch):\n    \"\"\"\n    Custom collate function to handle variable-size bounding box tensors\n    and convert them to the format required by the YOLO loss function.\n    \"\"\"\n    batch = [item for item in batch if item is not None]\n    if not batch: return None\n\n    collated = {}\n    # Collate standard tensors (rgb, ir)\n    collated['rgb'] = torch.stack([d['rgb'] for d in batch])\n    collated['ir'] = torch.stack([d['ir'] for d in batch])\n    # Collate other data as lists\n    collated['img_id'] = [d['img_id'] for d in batch]\n    collated['gt_boxes_xyxy'] = [d['bboxes'] for d in batch] # Keep original xyxy for visualization\n\n    # --- Create YOLO targets ---\n    # This is the required format for YOLOLoss: [batch_idx, class_idx, cx, cy, w, h]\n    yolo_targets_list = []\n    for i, d in enumerate(batch):\n        bboxes_xyxy = d['bboxes']\n        if bboxes_xyxy.shape[0] > 0:\n            # Convert from xyxy to cxcywh\n            bboxes_cxcywh = box_xyxy_to_cxcywh(bboxes_xyxy)\n            \n            # Create the target tensor for this image\n            num_boxes = bboxes_cxcywh.shape[0]\n            # [batch_idx, class_idx, cx, cy, w, h]\n            targets = torch.zeros((num_boxes, 6))\n            targets[:, 0] = i # Batch index\n            targets[:, 1] = 0 # Class index (0 for 'person')\n            targets[:, 2:] = bboxes_cxcywh\n            yolo_targets_list.append(targets)\n\n    # Concatenate all targets from the batch into a single tensor\n    if yolo_targets_list:\n        collated['yolo_targets'] = torch.cat(yolo_targets_list, 0)\n    else:\n        collated['yolo_targets'] = torch.zeros((0, 6))\n\n    return collated\n\ndef get_dataloader(root,split,bs,subset=None,img_sz=512,workers=2):\n    dset=LLVIPDataset(root,split,subset_size=subset,img_size=img_sz)\n    loader=DataLoader(dset,batch_size=bs,shuffle=(split=='train'),num_workers=workers,collate_fn=custom_collate_fn,pin_memory=True)\n    return loader\n\ndef visualize_attention_weights(rgb_tensor, ir_tensor, attention_weights, epoch_num, score_thresh=0.5):\n    \"\"\"\n    Visualizes the four spatial attention weights from the fusion module in a proper 2x3 grid.\n    \"\"\"\n    # Denormalize RGB for viewing\n    mean_rgb = np.array([0.485, 0.456, 0.406])\n    std_rgb = np.array([0.229, 0.224, 0.225])\n    rgb_img = np.clip(((rgb_tensor.cpu().numpy().transpose(1, 2, 0) * std_rgb) + mean_rgb), 0, 1)\n\n    # Denormalize IR for viewing (assuming [0.5], [0.5] normalization)\n    ir_img = np.clip(ir_tensor.cpu().numpy().squeeze() * 0.5 + 0.5, 0, 1)\n\n    # The weights tensor has shape [4, H, W]\n    weights = attention_weights.cpu().numpy()\n    weight_titles = [\n        'Weight on Fused RGB', 'Weight on Fused IR',\n        'Weight on Original RGB', 'Weight on Original IR'\n    ]\n\n    # --- CORRECTED PLOTTING LOGIC ---\n    fig, axes = plt.subplots(2, 3, figsize=(20, 12)) # Create a 2x3 grid\n    fig.suptitle(f'Spatial Attention Weights - Epoch {epoch_num}', fontsize=20)\n    \n    # Flatten the axes array for easy indexing\n    ax = axes.flat\n\n    # Plot Original Images in the first two slots\n    ax[0].imshow(rgb_img)\n    ax[0].set_title(\"Original RGB\")\n    ax[0].axis('off')\n\n    ax[1].imshow(ir_img, cmap='gray')\n    ax[1].set_title(\"Original IR\")\n    ax[1].axis('off')\n\n    # Plot the 4 Weight Heatmaps in the remaining slots\n    for i in range(4):\n        # Heatmaps will be placed in ax[2], ax[3], ax[4], and ax[5]\n        current_ax = ax[i + 2] \n        \n        # Upsample the heatmap for better visualization\n        heatmap = torch.from_numpy(weights[i]).unsqueeze(0).unsqueeze(0)\n        heatmap_upsampled = F.interpolate(heatmap, size=rgb_img.shape[:2], mode='bilinear', align_corners=False)\n        im = current_ax.imshow(heatmap_upsampled.squeeze().numpy(), cmap='viridis')\n        \n        current_ax.set_title(weight_titles[i])\n        current_ax.axis('off')\n        fig.colorbar(im, ax=current_ax, fraction=0.046, pad=0.04)\n\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.savefig(f\"attention_weights_epoch_{epoch_num}.png\")\n    plt.show()\n    print(f\"Saved attention weight visualization for epoch {epoch_num}.\")\n    \nfrom torchmetrics.detection import MeanAveragePrecision\n\n@torch.no_grad()\ndef evaluate(feature_extractor, yolo_head, criterion, val_loader, device, return_sample_for_vis=False):\n    feature_extractor.eval(); yolo_head.eval(); criterion.eval()\n    run_loss = 0\n    vis_sample = None\n    vis_idx = -1\n    if return_sample_for_vis and len(val_loader) > 0:\n        vis_idx = random.randint(0, len(val_loader) - 1)\n\n    # 1. Instantiate the metric object\n    # It will calculate mAP, mAP_50, mAP_75, etc.\n    metric = MeanAveragePrecision(box_format='xyxy').to(device)\n\n    for i, data in enumerate(val_loader):\n        if data is None: continue\n        rgb = data['rgb'].to(device, non_blocking=True)\n        ir = data['ir'].to(device, non_blocking=True)\n        yolo_targets = data['yolo_targets'].to(device)\n        gt_boxes_xyxy_list = data['gt_boxes_xyxy'] # List of tensors on CPU\n\n        # --- Forward pass and loss calculation (remains the same) ---\n        main_feats, _, spatial_weights = feature_extractor(rgb, ir)\n        preds_raw = yolo_head(main_feats)\n        if yolo_targets.shape[0] > 0:\n            loss_dict = criterion(preds_raw, yolo_targets)\n            run_loss += loss_dict['total_loss'].item()\n\n        # --- Decode predictions to get final boxes (same logic as for visualization) ---\n        yolo_head_module = yolo_head.module if isinstance(yolo_head, nn.DataParallel) else yolo_head\n        B, H, W, A, C = preds_raw.shape\n        preds_reshaped = preds_raw.view(B, H * W * A, C)\n        grid = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device), indexing='ij')\n        grid = torch.stack(grid, dim=-1).view(1, H, W, 1, 2).repeat(B, 1, 1, A, 1)\n        anchor_grid = yolo_head_module.anchor_grid.to(device).view(1, 1, 1, A, 2).repeat(B, H, W, 1, 1)\n        pred_xy = (preds_reshaped[..., 0:2].sigmoid() * 2 - 0.5 + grid.view(B, -1, 2)) * yolo_head_module.stride\n        pred_wh = (preds_reshaped[..., 2:4].sigmoid() * 2)**2 * anchor_grid.view(B, -1, 2) * yolo_head_module.stride\n        pred_cxcywh = torch.cat([pred_xy, pred_wh], dim=-1)\n        pred_cxcywh[..., 0::2] /= CONFIG['img_size']; pred_cxcywh[..., 1::2] /= CONFIG['img_size']\n        pred_for_nms = torch.cat((pred_cxcywh, preds_reshaped[..., 4:5].sigmoid(), preds_reshaped[..., 5:].sigmoid()), dim=-1)\n        final_preds_list = non_max_suppression(pred_for_nms, conf_thres=CONFIG['conf_threshold'], iou_thres=CONFIG['iou_threshold_nms'])\n\n        # 2. Format predictions and ground truths for the metric\n        preds_formatted = []\n        gts_formatted = []\n        for idx in range(B): # Iterate through each image in the batch\n            pred_boxes_scores = final_preds_list[idx] # Tensor of shape [N, 6] -> x1,y1,x2,y2,conf,class\n            preds_formatted.append({\n                \"boxes\": pred_boxes_scores[:, :4],\n                \"scores\": pred_boxes_scores[:, 4],\n                \"labels\": pred_boxes_scores[:, 5].int()\n            })\n\n            gt_boxes_for_img = gt_boxes_xyxy_list[idx].to(device)\n            gts_formatted.append({\n                \"boxes\": gt_boxes_for_img,\n                \"labels\": torch.zeros(gt_boxes_for_img.shape[0], dtype=torch.int, device=device) # All labels are 0\n            })\n\n        # 3. Update the metric with the current batch's data\n        metric.update(preds_formatted, gts_formatted)\n\n        if i == vis_idx:\n            vis_sample = {\n                'rgb': data['rgb'],\n                'ir': data['ir'],\n                'gt_boxes_xyxy': data['gt_boxes_xyxy'],\n                'final_preds': final_preds_list,\n                'attention_weights': spatial_weights['layer4'][0].detach().cpu() # Add this line\n            }\n    # 4. Compute the final metrics over the entire validation set\n    avg_loss = run_loss / len(val_loader) if len(val_loader) > 0 else 0\n    try:\n        map_results = {k: v.item() for k, v in metric.compute().items()}\n    except Exception as e:\n        print(f\"Could not compute mAP, likely no detections or GT boxes found in validation set. Error: {e}\")\n        map_results = {k: 0.0 for k in ['map', 'map_50', 'map_75', 'map_large', 'map_medium', 'map_small']}\n\n    if return_sample_for_vis:\n        return avg_loss, map_results, vis_sample\n    else:\n        return avg_loss, map_results, None\n        \n    \ndef train():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n    seed=42; random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n\n    data_root = '/kaggle/input/llvip-data'\n    if not os.path.exists(os.path.join(data_root, 'LLVIP')):\n        print(f\"Error: LLVIP data not found at {data_root}. Please check path.\"); return\n\n    train_loader = get_dataloader(data_root,'train',CONFIG['batch_size'],CONFIG['subset_size'],CONFIG['img_size'],CONFIG['num_workers'])\n    val_loader = get_dataloader(data_root, 'test', CONFIG['batch_size'], CONFIG['subset_size'] // 4, CONFIG['img_size'], CONFIG['num_workers'])\n    print(f\"Validation loader created with {len(val_loader.dataset)} samples.\")\n\n    # Initialize Models\n    feature_extractor = EnhancedHybridFeatureExtractor(use_vit_enhancer=False, use_axial_fusion=True, dropout_rate=CONFIG['dropout_rate']).to(device)\n    yolo_head = YOLOHead(\n        in_channels=CONFIG['d_model'],\n        num_classes=CONFIG['num_classes'],\n        anchors=CONFIG['anchors'][0],\n        stride=CONFIG['yolo_strides'][0]\n    ).to(device)\n\n    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs with nn.DataParallel.\")\n        feature_extractor = nn.DataParallel(feature_extractor); yolo_head = nn.DataParallel(yolo_head)\n        model_fe_opt = feature_extractor.module; model_yolo_opt = yolo_head.module\n    else:\n        model_fe_opt = feature_extractor; model_yolo_opt = yolo_head\n\n    FREEZE_EPOCHS = CONFIG['FREEZE_EPOCHS']\n    STABILIZATION_EPOCHS = 5\n    STABILIZATION_FACTOR = 100.0 # Reduce LR by 100x\n    model_fe_opt.freeze_backbone(freeze=True)\n    print(f\"Backbone is FULLY FROZEN for the first {FREEZE_EPOCHS} epochs.\")\n\n    # Loss Criterion\n    criterion = YOLOLoss(CONFIG).to(device)\n\n    # Optimizer\n    param_dicts = [\n        {\"params\": [p for n, p in model_fe_opt.named_parameters() if \"backbone\" in n and p.requires_grad], \"lr\": CONFIG['lr_backbone'], \"name\": \"backbone\"},\n        {\"params\": [p for n, p in model_fe_opt.named_parameters() if \"backbone\" not in n and p.requires_grad], \"lr\": CONFIG['lr'], \"name\": \"fusion_head\"},\n        {\"params\": model_yolo_opt.parameters(), \"lr\": CONFIG['lr'], \"name\": \"yolo_head\"},\n    ]\n    #optimizer = torch.optim.AdamW(param_dicts, weight_decay=CONFIG['weight_decay'])\n    optimizer = torch.optim.SGD(\n        param_dicts,\n        momentum=CONFIG['sgd_momentum'],\n        nesterov=True, # Nesterov momentum is often beneficial\n        weight_decay=CONFIG['weight_decay']\n    )\n    # Calculate the total number of optimizer steps\n    num_update_steps_per_epoch = math.ceil(len(train_loader) / CONFIG['ACCUMULATION_STEPS'])\n    num_training_steps = CONFIG['num_epochs'] * num_update_steps_per_epoch\n    # Set warmup to be the first epoch\n    num_warmup_steps = 1 * num_update_steps_per_epoch\n    # --- CHANGED: Use CosineAnnealingLR for a better learning rate schedule ---\n    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['num_epochs'], eta_min=1e-7)\n    # Create the scheduler with warmup\n    scheduler = get_scheduler(\n        \"cosine\",  # Use a cosine decay curve after warmup\n        optimizer=optimizer,\n        num_warmup_steps=num_warmup_steps,\n        num_training_steps=num_training_steps\n    )\n    print(f\"Scheduler created. Total steps: {num_training_steps}, Warmup steps: {num_warmup_steps}\")\n    scaler = GradScaler(enabled=torch.cuda.is_available())\n    best_loss=float('inf'); best_map = 0.0; no_improve_epochs=0; losses_hist=[]; val_losses_hist=[]\n    ACCUMULATION_STEPS = CONFIG['ACCUMULATION_STEPS']\n\n    print(f\"Starting training for {CONFIG['num_epochs']} epochs.\")\n    print(f\"Effective batch size: {CONFIG['batch_size'] * ACCUMULATION_STEPS}\")\n\n    for epoch in range(CONFIG['num_epochs']):\n        if epoch == FREEZE_EPOCHS: model_fe_opt.unfreeze_backbone_layer(\"layer4\")\n        elif epoch == FREEZE_EPOCHS+20: model_fe_opt.unfreeze_backbone_layer(\"layer3\")\n        elif epoch == FREEZE_EPOCHS+50: model_fe_opt.unfreeze_backbone_layer(\"layer2\")\n\n        feature_extractor.train(); yolo_head.train(); criterion.train()\n        run_loss=0; batch_count=0\n\n        for batch_idx, data in enumerate(train_loader):\n            if data is None: continue\n            rgb=data['rgb'].to(device, non_blocking=True); ir=data['ir'].to(device, non_blocking=True)\n            yolo_targets = data['yolo_targets'].to(device)\n\n            if yolo_targets.shape[0] == 0:\n                continue\n\n            with torch.amp.autocast(device_type=\"cuda\", enabled=torch.cuda.is_available()):\n                main_feats, deep_supervision_preds, spatial_weights  = feature_extractor(rgb, ir)\n                preds = yolo_head(main_feats)\n                loss_dict = criterion(preds, yolo_targets)\n                total_loss_yolo = loss_dict['total_loss']\n                total_loss = total_loss_yolo\n\n                if epoch >= CONFIG['FREEZE_EPOCHS']:\n                    # Calculate Deep Supervision Loss\n                    ds_targets_list = [{'gt_boxes_xyxy': b} for b in data['gt_boxes_xyxy']]\n                    \n                    # Pass the correct variable and the device to the function\n                    ds_target_labels, _ = prepare_deep_supervision_targets(\n                        ds_targets_list, \n                        CONFIG['backbone_feature_sizes'][\"layer4\"], \n                        device\n                    )\n\n                    loss_ds_ce = F.cross_entropy(deep_supervision_preds['pred_logits'], ds_target_labels)\n                    total_loss = total_loss + CONFIG['lambda_deep_supervision'] * loss_ds_ce\n\n\n                if not math.isfinite(total_loss.item()):\n                    print(f\"ERROR: Non-finite loss {total_loss.item()} at batch {batch_idx}. Skipping.\"); continue\n\n                loss = total_loss / ACCUMULATION_STEPS\n                scaler.scale(loss).backward()\n\n            if (batch_idx + 1) % ACCUMULATION_STEPS == 0:\n                if scaler.is_enabled():\n                    scaler.unscale_(optimizer)\n                    \n                optimizer_step = (batch_idx + 1) // ACCUMULATION_STEPS\n                if optimizer_step % 20 == 0: # Print every 50 optimizer steps\n                    print(\"\\n--- Before Clipping ---\")\n                    print_grad_norms(feature_extractor, \"FeatureExtractor\")\n                    print_grad_norms(yolo_head, \"YOLOHead\")\n                torch.nn.utils.clip_grad_norm_(optimizer.param_groups[0]['params'], CONFIG['clip_norm_backbone'])\n                torch.nn.utils.clip_grad_norm_(optimizer.param_groups[1]['params'], CONFIG['clip_norm_backbone'])\n                torch.nn.utils.clip_grad_norm_(optimizer.param_groups[2]['params'], CONFIG['clip_norm_head'])\n                \n                scaler.step(optimizer)\n                scaler.update()\n                scheduler.step() # new scheduler\n                # This happens *after* the scheduler step to ensure our value is the final one.\n                if  (CONFIG['FREEZE_EPOCHS'] <= epoch < CONFIG['FREEZE_EPOCHS'] + STABILIZATION_EPOCHS) or \\\n                    (CONFIG['FREEZE_EPOCHS'] + 20 <= epoch < CONFIG['FREEZE_EPOCHS'] + 20 + STABILIZATION_EPOCHS) or \\\n                    (CONFIG['FREEZE_EPOCHS'] + 50 <= epoch < CONFIG['FREEZE_EPOCHS'] + 50 + STABILIZATION_EPOCHS):\n                    \n                    # Get the learning rate that the scheduler just set for the backbone\n                    scheduled_lr = scheduler.get_last_lr()[0] \n                    \n                    # Override it with our much smaller, stabilized value\n                    optimizer.param_groups[0]['lr'] = scheduled_lr / STABILIZATION_FACTOR\n\n                optimizer.zero_grad(set_to_none=True)\n\n            run_loss += total_loss.item(); batch_count += 1\n            if len(train_loader)>0 and (batch_idx%100==0 or batch_idx==len(train_loader)-1):\n                lr_bb, lr_head_fusion, lr_head_yolo = optimizer.param_groups[0]['lr'], optimizer.param_groups[1]['lr'], optimizer.param_groups[2]['lr']\n                loss_box_val = loss_dict['loss_box'].item()\n                loss_obj_val = loss_dict['loss_obj'].item()\n                loss_cls_val = loss_dict['loss_cls'].item()\n                log_msg = (f\"[Epoch {epoch+1}/{CONFIG['num_epochs']} Batch {batch_idx}/{len(train_loader)}] \"\n                           f\"Total Loss:{total_loss.item():.4f} | \"\n                           f\"Box: {loss_box_val:.4f}, Obj: {loss_obj_val:.4f}, Cls: {loss_cls_val:.4f}\")\n                \n                if epoch >= FREEZE_EPOCHS:\n                    log_msg += f\", DS: {loss_ds_ce:.4f}\" # Add DS loss to the log\n                \n                log_msg += (f\" | LRs -> BB: {lr_bb:.2e}, Fusion: {lr_head_fusion:.2e}, YOLO: {lr_head_yolo:.2e}\")\n                print(log_msg)\n\n        # Step the scheduler at the end of each epoch\n        #scheduler.step()\n\n        avg_loss = run_loss / batch_count if batch_count > 0 else float('inf')\n        losses_hist.append(avg_loss)\n        print(f\"\\n--- Epoch {epoch+1} Finished --- Average Training Loss: {avg_loss:.4f} ---\")\n\n        is_vis_epoch = (epoch + 1) % 1 == 0 or epoch == CONFIG['num_epochs'] - 1 # Visualize every 5 epochs\n        val_loss, val_metrics, vis_val_sample = evaluate(feature_extractor, yolo_head, criterion, val_loader, device, return_sample_for_vis=is_vis_epoch)\n        # Print the results\n        print(f\"--- Validation Loss: {val_loss:.4f} ---\")\n        print(f\"--- Validation mAP@.50:.95: {val_metrics['map']:.4f} | mAP@.50: {val_metrics['map_50']:.4f} | mAP@.75: {val_metrics['map_75']:.4f} ---\")\n        val_losses_hist.append(val_loss)\n\n        \"\"\"if 'data' in locals():\n            print(\"Generating training visualization...\")\n            feature_extractor.eval(); yolo_head.eval()\n            with torch.no_grad():\n                try:\n                    # Run the last training batch through the model\n                    main_feats, _, _ = feature_extractor(data['rgb'].to(device), data['ir'].to(device))\n                    preds = yolo_head(main_feats)\n                    \n                    # Decode predictions for visualization\n                    yolo_head_module = yolo_head.module if isinstance(yolo_head, nn.DataParallel) else yolo_head\n                    B, H, W, A, C = preds.shape\n                    preds_reshaped = preds.view(B, H * W * A, C)\n                    grid = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device), indexing='ij')\n                    grid = torch.stack(grid, dim=-1).view(1, H, W, 1, 2).repeat(B, 1, 1, A, 1)\n                    anchor_grid = yolo_head_module.anchor_grid.to(device).view(1, 1, 1, A, 2).repeat(B, H, W, 1, 1)\n                    pred_xy = (preds_reshaped[..., 0:2].sigmoid() * 2 - 0.5 + grid.view(B, -1, 2)) * yolo_head_module.stride\n                    pred_wh = (preds_reshaped[..., 2:4].sigmoid() * 2)**2 * anchor_grid.view(B, -1, 2) * yolo_head_module.stride\n                    pred_cxcywh = torch.cat([pred_xy, pred_wh], dim=-1)\n                    pred_cxcywh[..., 0::2] /= CONFIG['img_size']; pred_cxcywh[..., 1::2] /= CONFIG['img_size']\n                    pred_for_nms = torch.cat((pred_cxcywh, preds_reshaped[..., 4:5].sigmoid(), preds_reshaped[..., 5:].sigmoid()), dim=-1)\n                    final_preds_train = non_max_suppression(pred_for_nms, conf_thres=CONFIG['conf_threshold'], iou_thres=CONFIG['iou_threshold_nms'])\n\n                    # Visualize the first image in the batch\n                    visualize_boxes(data['rgb'][0].detach().cpu(), final_preds_train[0].detach().cpu(), data['gt_boxes_xyxy'][0].detach().cpu(), score_thresh=CONFIG['conf_threshold'], title_suffix=f\"TRAINING Sample - Epoch {epoch+1}\")\n                    # Prepare the IR image for visualization (convert 1-channel to 3-channel)\n                    ir_img_vis = data['ir'][0].detach().cpu().repeat(3, 1, 1)\n                    # Visualize the IR image with the same predictions\n                    visualize_boxes(ir_img_vis, final_preds_train[0].detach().cpu(), data['gt_boxes_xyxy'][0].detach().cpu(), score_thresh=CONFIG['conf_threshold'], denormalize=False, title_suffix=f\"TRAINING IR Sample - Epoch {epoch+1}\")\n                except Exception as e:\n                    print(f\"Training visualization failed: {e}\")\"\"\"\n                    \n        if is_vis_epoch and vis_val_sample:\n            print(\"Generating validation visualization...\")\n            try:\n                vis_rgb_val = vis_val_sample['rgb'][0].detach().cpu()\n                vis_gt_boxes_val = vis_val_sample['gt_boxes_xyxy'][0].detach().cpu()\n                vis_pred_boxes_val = vis_val_sample['final_preds'][0].detach().cpu()\n                visualize_boxes(vis_rgb_val, vis_pred_boxes_val, vis_gt_boxes_val, score_thresh=CONFIG['conf_threshold'], title_suffix=f\"VALIDATION Sample - Epoch {epoch+1}\")\n            except Exception as e:\n                print(f\"Validation visualization failed: {e}\")\n            print(\"Generating attention weight visualization...\")\n            try:\n                visualize_attention_weights(\n                    rgb_tensor=vis_val_sample['rgb'][0],\n                    ir_tensor=vis_val_sample['ir'][0],\n                    attention_weights=vis_val_sample['attention_weights'],\n                    epoch_num=epoch + 1\n                )\n            except Exception as e:\n                print(f\"Attention visualization failed: {e}\")\n\n        if val_loss < best_loss:\n            print(f\"Validation loss improved ({best_loss:.4f} --> {val_loss:.4f}). Saving model...\")\n            best_loss = val_loss\n            no_improve_epochs = 0\n            save_dir = \"./checkpoints_yolo\"\n            os.makedirs(save_dir, exist_ok=True)\n            torch.save(model_fe_opt.state_dict(), os.path.join(save_dir, 'feature_extractor_best.pth'))\n            torch.save(model_yolo_opt.state_dict(), os.path.join(save_dir, 'yolo_head_best.pth'))\n        else:\n            no_improve_epochs += 1\n            print(f\"Validation loss did not improve. Epochs w/o improvement: {no_improve_epochs}/{CONFIG['early_stop_patience']}\")\n            if no_improve_epochs >= CONFIG['early_stop_patience']:\n                print(\"Early stopping triggered.\"); break\n        # --- MODIFICATION: Save model based on best mAP@.50 ---\n        current_map50 = val_metrics['map_50']\n        if current_map50 > best_map:\n            print(f\"Validation mAP@.50 improved ({best_map:.4f}).\")\n            best_map = current_map50\n            \"\"\"no_improve_epochs = 0\n            save_dir = \"./checkpoints_yolo\"\n            os.makedirs(save_dir, exist_ok=True)\n            torch.save(model_fe_opt.state_dict(), os.path.join(save_dir, 'feature_extractor_best.pth'))\n            torch.save(model_yolo_opt.state_dict(), os.path.join(save_dir, 'yolo_head_best.pth'))\n        else:\n            no_improve_epochs += 1\n            print(f\"Validation mAP@.50 did not improve. Epochs w/o improvement: {no_improve_epochs}/{CONFIG['early_stop_patience']}\")\n            if no_improve_epochs >= CONFIG['early_stop_patience']:\n                print(\"Early stopping triggered.\"); break\"\"\"\n                \n    if losses_hist and val_losses_hist:\n        plt.figure(figsize=(12, 6)); plt.plot(losses_hist, 'b-o', label='Training Loss'); plt.plot(val_losses_hist, 'r-o', label='Validation Loss')\n        plt.title(\"Training & Validation Loss Progression\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Average Loss\"); plt.legend(); plt.grid(True)\n        plt.savefig(\"training_validation_loss_curve_yolo.png\"); print(\"Saved training and validation loss curve.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == '__main__':\n    train()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}